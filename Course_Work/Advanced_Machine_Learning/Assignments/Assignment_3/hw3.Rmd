---
title: "hw3"
author: "Norman Hong"
date: "April 9, 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
```


# Q0.) Using rpart to build a gradient boosted tree with v=0.05 and no changes in the default rpart parameters.
```{r}
# Generating sample data
n=300
set.seed(1)
u=sort(runif(n)*5*pi)
y = sin(u)+rnorm(n)/4
df = data.frame(x=u,y=y)

temp1 <- function(v, number_of_weak_learners=100){
  # v is the learning rate.
  # number_of_weak_learners is the number of trees in boosted model. 
  
  # Fit first iteration
  fit = rpart(y ~ x, data=df)  # Regression tree.
  yp = predict(fit,newdata=df)
  df$yr = df$y - v*yp  # Update to residuals in boosting.
  YP = v*yp # This the spline prediction multiplied by a weight, known as the learning rate.
  list_of_weak_learners = list(fit)

  # Fitting rest of weak learners. 
  for(t in 2:number_of_weak_learners){
    # Fit regression tree.
    fit = rpart(yr ~ x, data=df)
    
    # Generate new prediction
    yp=predict(fit,newdata=df)
    
    # Update residuals
    df$yr=df$yr - v*yp
    
    # Bind to new data point; Adding v*yp to YP
    # Each column is a weak learner. Rows are the data points.
    YP = cbind(YP,v*yp)
    
    # Store fitted model in list
  list_of_weak_learners[[t]] = fit
  }
  return(list_of_weak_learners)
}

boosted_model <- temp1(.05)
```

# Q1.) What happens when you change v, the learning parameter? Show the fitted plots v=.01, .05, .125.
```{r}
temp2 <- function(v, number_of_weak_learners=100){
  # v is the learning rate.
  
  # Fit first iteration
  fit = rpart(y ~ x, data=df)  # Regression tree.
  yp = predict(fit,newdata=df)
  df$yr = df$y - v*yp  # Update to residuals in boosting.
  YP = v*yp # This the spline prediction multiplied by a weight, known as the learning rate.
  list_of_weak_learners = list(fit)
  
  # Fitting rest of weak learners. 
  for(t in 2:number_of_weak_learners){
    # Fit regression tree
    fit = rpart(yr ~ x, data=df)
    
    # Generate new prediction
    yp=predict(fit,newdata=df)
    
    # Update residuals
    df$yr=df$yr - v*yp
    
    # Bind to new data point; Adding v*yp to YP
    # Each column is a weak learner. Rows are the data points.
    YP = cbind(YP,v*yp)
    
    # Store fitted model in list
  list_of_weak_learners[[t]] = fit
  } 

# Getting predictions for each boost
# Iterate through all weak learners. 
for (i in 1:number_of_weak_learners){
  # Calculating performance of first i weak_learners
  
  # Summing weak learner residuals
  # Summing up the predictions across all learners for 
  # each data point. Final output is last i.
  if(i==1){yp_i = YP[,1:i]
  }else{yp_i=apply(YP[,1:i],1,sum) #<- strong learner
  }
  
  # Binds new cols
  col_name = paste0('yp_',i)  # TODO: not sure what the point of this is. It isn't used.
  # yp, yp1, yp2, ... column names are auto generated by r. 
  df = df %>% bind_cols(yp=yp_i)  # Adding all of yp to df on columns axis. 
}

# Re-arrange sequences to get pseudo residuals 
plot_wl = df %>% select(-y,-yr) %>%  # remove y and yr column. 
  pivot_longer(cols = starts_with("yp")) %>%  # Transpose matrix containing all "yp" in column names. 
  mutate(learner = str_match(name,"[0-9]+")) %>%  # Create learner column with values based on str_match. 
  mutate(learner = as.integer(ifelse(is.na(learner),0,learner))) # Turn the Na value into 0. 

# final learner
final_learner = plot_wl %>% filter(learner == (number_of_weak_learners-1))  # Selecting the final learner, 99.

# Plot progression of learner
ggplot() + 
  # Visualizing all learners
  geom_line(aes(x = x, y = value, group = learner, color =learner),
            data = plot_wl,alpha=0.5) +
  # Final learner
  geom_line(aes(x = x, y = value, group = learner, color =learner),
            data = final_learner,alpha=0.5,color = 'firebrick1',size = 2)  +
  geom_point(aes(x = x, y= y),data = df)+ # true values (data points)
  theme_minimal() + 
  ggtitle(paste0("Learning Rate: ", v))

}
temp2(.05)
temp2(.01)
temp2(.125)
```

# Q2.) Using a validation and test set, develop a heuristic approach for determining when to stop training your gradient boosted tree using v=.05 and the default setting, how many trees did you include, and what is your performance on the test set for the set of selected parameters (use RMSE). 

55 trees seem to be optimal with a performance of .0934 RMSE.
```{r}
set.seed(1)
boosted_model <- temp1(.05)

temp3 <- function(new_data, list_of_weak_learners, v=.05, number_of_weak_learners = 100){
  # Doing predictions on new data.
  # number_of_weak_learners must be the same number used in temp1() and temp2(). 
  for (i in 1:number_of_weak_learners){
    weak_learner_i = list_of_weak_learners[[i]]
    
    if (i==1){pred = v*predict(weak_learner_i,new_data)}
    else{pred =pred + v*predict(weak_learner_i,new_data)}
    
    if(i==number_of_weak_learners){
      new_data = new_data %>% bind_cols(yp=pred)
    }
  }
  return(new_data)
}
rmse <- function(y_true, y_pred){
  # Calculates root mean squared error. 
  sq <- (y_true - y_pred)^2
  out <- sqrt(mean(sq))
  return(out)
}
```

```{r}
# Generate new data
set.seed(1)
x = sample(seq(0,4*3,0.001),size = 100,replace = T)
y = sin(x)
validation_data <- data.frame(x=x,y=y)

# validation rmse; trees = 125
boosted_model <- temp1(v=.05, number_of_weak_learners=125)
vd <- temp3(validation_data, boosted_model, number_of_weak_learners=125)
cat('\ntrees: 120 --> RMSE: ', rmse(vd$y, vd$yp))

# validation rmse; trees = 110
boosted_model <- temp1(v=.05, number_of_weak_learners=110)
vd <- temp3(validation_data, boosted_model, number_of_weak_learners=110)
cat('\ntrees: 110 --> RMSE: ', rmse(vd$y, vd$yp))

# validation rmse; trees = 100
boosted_model <- temp1(v=.05, number_of_weak_learners=100)
vd <- temp3(validation_data, boosted_model, number_of_weak_learners=100)
cat('\ntrees: 100 --> RMSE: ', rmse(vd$y, vd$yp))

# validation rmse; trees = 70
boosted_model <- temp1(v=.05, number_of_weak_learners=70)
vd <- temp3(validation_data, boosted_model, number_of_weak_learners=70)
cat('\ntrees: 70 --> RMSE: ', rmse(vd$y, vd$yp))

# validation rmse; trees = 60
boosted_model <- temp1(v=.05, number_of_weak_learners=60)
vd <- temp3(validation_data, boosted_model, number_of_weak_learners=60)
cat('\ntrees: 60 --> RMSE: ', rmse(vd$y, vd$yp))

# validation rmse; trees = 55
boosted_model <- temp1(v=.05, number_of_weak_learners=55)
vd <- temp3(validation_data, boosted_model, number_of_weak_learners=55)
cat('\ntrees: 55 --> RMSE: ', rmse(vd$y, vd$yp))

# validation rmse; trees = 52
boosted_model <- temp1(v=.05, number_of_weak_learners=52)
vd <- temp3(validation_data, boosted_model, number_of_weak_learners=52)
cat('\ntrees: 52 --> RMSE: ', rmse(vd$y, vd$yp))

# trees = 50
boosted_model <- temp1(v=.05, number_of_weak_learners=50)
vd <- temp3(validation_data, boosted_model, number_of_weak_learners=50)
cat('\ntrees: 50 --> RMSE: ', rmse(vd$y, vd$yp))

# trees = 40
boosted_model <- temp1(v=.05, number_of_weak_learners=40)
vd <- temp3(validation_data, boosted_model, number_of_weak_learners=40)
cat('\ntrees: 40 --> RMSE: ', rmse(vd$y, vd$yp))

# trees = 25
boosted_model <- temp1(v=.05, number_of_weak_learners=25)
vd <- temp3(validation_data, boosted_model, v=.05, number_of_weak_learners=25)
cat('\ntrees: 25 --> RMSE: ', rmse(vd$y, vd$yp))


```

# Q3.) Next, you are going to tune your trees. Use grid search to assess different values for minsplit, cp, maxdepth. What is the best set of parameters as measured by RMSE?
minsplit of 60, cp of 0.01, maxdepth of 5 give the lowest rmse. 
```{r}
temp1 <- function(v=.05, number_of_weak_learners=55, minsplit=20, cp=.01, maxdepth=30){
  # v is the learning rate.
  # number_of_weak_learners is the number of trees in boosted model.
  # minsplit is the minimum number of data points in a node where if smaller, theen can't split anymore. 
  # cp is the value that a split must decrease the loss function by.
  # maxdepth is the maximum depth of a tree.  
  
  # Fit first iteration
  controls = rpart.control(minsplit = minsplit,
                minbucket = round(minsplit/3),
                cp = cp,
                maxcompete = 4, maxsurrogate = 5,
                usesurrogate = 2, xval = 10,
                surrogatestyle = 0, maxdepth = maxdepth)
  fit = rpart(y ~ x, data=df, control=controls)  # Regression tree.
  yp = predict(fit, newdata=df)
  df$yr = df$y - v*yp  # Update to residuals in boosting.
  YP = v*yp # This the spline prediction multiplied by a weight, known as the learning rate.
  list_of_weak_learners = list(fit)

  # Fitting rest of weak learners. 
  for(t in 2:number_of_weak_learners){
    # Fit regression tree.
    fit = rpart(yr ~ x, data=df)
    
    # Generate new prediction
    yp=predict(fit,newdata=df)
    
    # Update residuals
    df$yr=df$yr - v*yp
    
    # Bind to new data point; Adding v*yp to YP
    # Each column is a weak learner. Rows are the data points.
    YP = cbind(YP,v*yp)
    
    # Store fitted model in list
  list_of_weak_learners[[t]] = fit
  }
  return(list_of_weak_learners)
}
temp3 <- function(new_data, list_of_weak_learners, v=.05, number_of_weak_learners = 55){
  # Doing predictions on new data.
  # number_of_weak_learners must be the same number used in temp1() and temp2(). 
  for (i in 1:number_of_weak_learners){
    weak_learner_i = list_of_weak_learners[[i]]
    
    if (i==1){pred = v*predict(weak_learner_i,new_data)}
    else{pred =pred + v*predict(weak_learner_i,new_data)}
    
    if(i==number_of_weak_learners){
      new_data = new_data %>% bind_cols(yp=pred)
    }
  }
  return(new_data)
}
rmse <- function(y_true, y_pred){
  # Calculates root mean squared error. 
  sq <- (y_true - y_pred)^2
  out <- sqrt(mean(sq))
  return(out)
}
gridsearch <- function(){
  minsplit = seq(10, 100, 10)
  cp = seq(.01, 10, 1)
  maxdepth = seq(2, 8, 1)
  results =  rep(NA, length(minsplit) * length(cp) * length(maxdepth))
  params = rep(NA, length(minsplit) * length(cp) * length(maxdepth))
  index = 1 # tracks index for results
  for(i in minsplit){
    for(j in cp){
      for(k in maxdepth){
        boosted_model <- temp1(v=.05, number_of_weak_learners=55, minsplit=i, cp=j, maxdepth=k)
        vd <- temp3(validation_data, boosted_model, v=.05, number_of_weak_learners=55)
        out = rmse(vd$y, vd$yp)
        # cat('\n','minsplit: ', i, 'cp: ', j, 'maxdepth: ', k, 'RMSE: ', out)
        results[index] = out
        params[index] = paste0('minsplit: ', i, ', cp: ', j, ', maxdepth: ', k) # c(minsplit, cp, maxdepth)
        index = index + 1
      }
    }
  }
  return(c(results, params))
}
```

```{r}
out <- gridsearch()
results <- out[1:700]
params <- out[701:1400]
results[which.min(results)]
params[which.min(results)]
```


# Q5: The approach above can be extendewd to multiple dimensions. Repeat Q1 to Q3 for the data set kernel_regression_2.csv from Assignment 2. 

```{r}
data <- read.csv("kernel_regression_2.csv")  # Z is the target, x and y are the inputs.

## 75% of the sample size
smp_size <- floor(0.75 * nrow(data))

## set the seed to make your partition reproducible
set.seed(1)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)

df <- data[train_ind, ]
validation_data <- data[-train_ind, ]
```

```{r}
temp1 <- function(v=.05, number_of_weak_learners=55, minsplit=20, cp=.01, maxdepth=30){
  # v is the learning rate.
  # number_of_weak_learners is the number of trees in boosted model.
  # minsplit is the minimum number of data points in a node where if smaller, theen can't split anymore. 
  # cp is the value that a split must decrease the loss function by.
  # maxdepth is the maximum depth of a tree.  
  
  # Fit first iteration
  controls = rpart.control(minsplit = minsplit,
                minbucket = round(minsplit/3),
                cp = cp,
                maxcompete = 4, maxsurrogate = 5,
                usesurrogate = 2, xval = 10,
                surrogatestyle = 0, maxdepth = maxdepth)
  fit = rpart(z ~ x + y, data=df, control=controls)  # Regression tree.
  zp = predict(fit, newdata=df)
  df$zr = df$z - v*zp  # Update to residuals in boosting.
  ZP = v*zp # This the spline prediction multiplied by a weight, known as the learning rate.
  list_of_weak_learners = list(fit)

  # Fitting rest of weak learners. 
  for(t in 2:number_of_weak_learners){
    # Fit regression tree.
    fit = rpart(zr ~ x+y, data=df)
    
    # Generate new prediction
    zp=predict(fit,newdata=df)
    
    # Update residuals
    df$zr=df$zr - v*zp
    
    # Bind to new data point; Adding v*yp to YP
    # Each column is a weak learner. Rows are the data points.
    ZP = cbind(ZP,v*zp)
    
    # Store fitted model in list
  list_of_weak_learners[[t]] = fit
  }
  return(list_of_weak_learners)
}
temp3 <- function(new_data, list_of_weak_learners, v=.05, number_of_weak_learners = 55){
  # Doing predictions on new data.
  # number_of_weak_learners must be the same number used in temp1() and temp2(). 
  for (i in 1:number_of_weak_learners){
    weak_learner_i = list_of_weak_learners[[i]]
    
    if (i==1){pred = v*predict(weak_learner_i,new_data)}
    else{pred =pred + v*predict(weak_learner_i,new_data)}
    
    if(i==number_of_weak_learners){
      new_data = new_data %>% bind_cols(zp=pred)
    }
  }
  return(new_data)
}
rmse <- function(y_true, y_pred){
  # Calculates root mean squared error. 
  sq <- (y_true - y_pred)^2
  out <- sqrt(mean(sq))
  return(out)
}
gridsearch <- function(){
  minsplit = seq(10, 100, 10)
  cp = seq(.01, 10, 1)
  maxdepth = seq(2, 8, 1)
  number_of_weak_learners = seq(80, 100, 20)
  results =  rep(NA, length(minsplit) * length(cp) * length(maxdepth))
  params = rep(NA, length(minsplit) * length(cp) * length(maxdepth))
  index = 1 # tracks index for results
  for(i in minsplit){
    for(j in cp){
      for(k in maxdepth){
        for(l in number_of_weak_learners){
          boosted_model <- temp1(v=.05, number_of_weak_learners=l, minsplit=i, cp=j, maxdepth=k)
          vd <- temp3(validation_data, boosted_model, v=.05, number_of_weak_learners=l)
          out = rmse(vd$z, vd$zp)
          # cat('\n','minsplit: ', i, 'cp: ', j, 'maxdepth: ', k, 'number of trees: ', l, 'RMSE: ', out)
          results[index] = out
          params[index] = paste0('minsplit: ', i, ', cp: ', j, ', maxdepth: ', k, ', number of trees: ', l) # c(minsplit, cp, maxdepth)
          index = index + 1 
        }
      }
    }
  }
  return(c(results, params))
}
```

```{r}
out <- gridsearch()
```
```{r}
results <- out[1:1400]
params <- out[1401:2800]
results[which.min(results)]
params[which.min(results)]
```


# Question 2 (TSNE)
## Part 1
## a.) Do the distances between points in tSNE matter?
The notion of distance changes depending on regional density variation. tSNE expands dense clusters and contracts sparse ones. As a result, clusters in tSNE representations looks approximately the same in size. 

## b.) What does the parameter value 'perplexity' mean?
It determines how to balance attention between local and global aspects of the data. It is a guess about the number of close neighbors each point has. Varying perplexity between 5 and 50 will cause nuanced changes to the plot. Each plot won't look exactly the same, but looking for common structure across multiple plots in this range might elude to the underlying structure of the data. 

## c.) What effect does the number of steps have on the final outcome of embedding?
The number of iterations ensures tSNE find a stable form. 

## d.) Explain why you may need more than one plot to explain topological information using tSNE.
tSNE is a very flexible method that can do all sorts of transformations and bring out different structure in data when there is no underlying structure. Looking across different plots will help elude the user to the true underlying structure. For instance, changing perplexity alone will result in a wide variety of different plots. 

## Part 2.

```{r}
library(tidyverse)
library(Rtsne)
library(RColorBrewer)
```

```{r}
# Get MNIST data
mnist_raw <- read_csv("https://pjreddie.com/media/files/mnist_train.csv", col_names = FALSE)

first_10k_samples =  mnist_raw[1:10000,-1] #%>% as.matrix()
first_10k_samples_labels =  mnist_raw[1:10000,1] %>% unlist(use.names=F)
colors = brewer.pal(10, 'Spectral')
```

## a.) Plot the PCA plot.
```{r}
pca = princomp(first_10k_samples)$scores[,1:2]
pca_plot = tibble(x = pca[,1], y =pca[,2], labels = as.character(first_10k_samples_labels))
ggplot(aes(x = x, y=y,label = labels, color = labels), data = pca_plot) + geom_text() + 
  xlab('PCA component 1') +ylab('PCA component 2')
```

## b.) Plot the TSNE embedding for perplexity = 5 and use 500 iterations. 

```{r}
temp <- c()
perp <- c()
pep <- 5
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                  perplexity = pep, 
                  theta = 0.5, 
                  eta = 200, # learning rate. 
                  pca = TRUE, verbose = TRUE, 
                  max_iter = 500, # number of iterations.
                  num_threads=0) 

plot_emb <- function(embedding, pep){
  # Wrapper to plot embedding.
  embedding_plot = tibble(x = embedding$Y[,1], y = embedding$Y[,2], 
                        labels = as.character(first_10k_samples_labels))
  ggplot(aes(x = x, y=y,label = labels, color = labels), data = embedding_plot) + 
    geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"') +
    ggtitle(paste0("perplexity: ",pep))
}
plot_emb(embedding, pep)
temp <- append(temp, embedding$itercosts[length(embedding$itercosts)])
perp <- append(perp, pep)
```

## c.) Plot the TSNE embedding for perplexity =5,20,60,100,125,160, what do you notice?

Can see a gap appears between clusters and clusters are being stretched or rotated on the x-axis, tSNE dimension 1.
```{r}
pep <- 20
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                  perplexity = pep, 
                  theta = 0.5, 
                  eta = 200, # learning rate. 
                  pca = TRUE, verbose = TRUE, 
                  max_iter = 500, # number of iterations.
                  num_threads=0) 

temp <- append(temp, embedding$itercosts[length(embedding$itercosts)])
perp <- append(perp, pep)

plot_emb <- function(embedding, pep){
  # Wrapper to plot embedding.
  embedding_plot = tibble(x = embedding$Y[,1], y = embedding$Y[,2], 
                        labels = as.character(first_10k_samples_labels))
  ggplot(aes(x = x, y=y,label = labels, color = labels), data = embedding_plot) + 
    geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"') +
    ggtitle(paste0("perplexity: ",pep))
}
plot_emb(embedding, pep)

pep <- 60
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                  perplexity = pep, 
                  theta = 0.5, 
                  eta = 200, # learning rate. 
                  pca = TRUE, verbose = TRUE, 
                  max_iter = 500, # number of iterations.
                  num_threads=0) 

temp <- append(temp, embedding$itercosts[length(embedding$itercosts)])
perp <- append(perp, pep)

plot_emb <- function(embedding, pep){
  # Wrapper to plot embedding.
  embedding_plot = tibble(x = embedding$Y[,1], y = embedding$Y[,2], 
                        labels = as.character(first_10k_samples_labels))
  ggplot(aes(x = x, y=y,label = labels, color = labels), data = embedding_plot) + 
    geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"') +
    ggtitle(paste0("perplexity: ",pep))
}
plot_emb(embedding, pep)

pep <- 100
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                  perplexity = pep, 
                  theta = 0.5, 
                  eta = 200, # learning rate. 
                  pca = TRUE, verbose = TRUE, 
                  max_iter = 500, # number of iterations.
                  num_threads=0)

temp <- append(temp, embedding$itercosts[length(embedding$itercosts)])
perp <- append(perp, pep)

plot_emb <- function(embedding, pep){
  # Wrapper to plot embedding.
  embedding_plot = tibble(x = embedding$Y[,1], y = embedding$Y[,2], 
                        labels = as.character(first_10k_samples_labels))
  ggplot(aes(x = x, y=y,label = labels, color = labels), data = embedding_plot) + 
    geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"') +
    ggtitle(paste0("perplexity: ",pep))
}
plot_emb(embedding, pep)

pep <- 125
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                  perplexity = pep, 
                  theta = 0.5, 
                  eta = 200, # learning rate. 
                  pca = TRUE, verbose = TRUE, 
                  max_iter = 500, # number of iterations.
                  num_threads=0) 

temp <- append(temp, embedding$itercosts[length(embedding$itercosts)])
perp <- append(perp, pep)

plot_emb <- function(embedding, pep){
  # Wrapper to plot embedding.
  embedding_plot = tibble(x = embedding$Y[,1], y = embedding$Y[,2], 
                        labels = as.character(first_10k_samples_labels))
  ggplot(aes(x = x, y=y,label = labels, color = labels), data = embedding_plot) + 
    geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"') +
    ggtitle(paste0("perplexity: ",pep))
}
plot_emb(embedding, pep)

pep <- 160
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                  perplexity = pep, 
                  theta = 0.5, 
                  eta = 200, # learning rate. 
                  pca = TRUE, verbose = TRUE, 
                  max_iter = 500, # number of iterations.
                  num_threads=0) 

temp <- append(temp, embedding$itercosts[length(embedding$itercosts)])
perp <- append(perp, pep)

plot_emb <- function(embedding, pep){
  # Wrapper to plot embedding.
  embedding_plot = tibble(x = embedding$Y[,1], y = embedding$Y[,2], 
                        labels = as.character(first_10k_samples_labels))
  ggplot(aes(x = x, y=y,label = labels, color = labels), data = embedding_plot) + 
    geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"') +
    ggtitle(paste0("perplexity: ",pep))
}
plot_emb(embedding, pep)
```

## d.) If the perplexity is set to 1, what would the distribution of values look like in 2d, provide an explanation as to why.

The distribution of value will look random and uniform. Perplexity represents the number of neighbors used in manifold learning. This means that a perplexity of 1 causes the algorithm to use only 1 neighbor. Therefore, it would be hard, in large datasets, to find patterns or underlying structure. 
```{r}
pep <- 1
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                  perplexity = pep, 
                  theta = 0.5, 
                  eta = 200, # learning rate. 
                  pca = TRUE, verbose = TRUE, 
                  max_iter = 500, # number of iterations.
                  num_threads=0) 

plot_emb <- function(embedding, pep){
  # Wrapper to plot embedding.
  embedding_plot = tibble(x = embedding$Y[,1], y = embedding$Y[,2], 
                        labels = as.character(first_10k_samples_labels))
  ggplot(aes(x = x, y=y,label = labels, color = labels), data = embedding_plot) + 
    geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"') +
    ggtitle(paste0("perplexity: ",pep))
}
plot_emb(embedding, pep)
```

## e.) How about if the perplexity is set to 5000? What would the distribut ion of values look like in 2d? Provide an explanation as to why. Note, don't try this on computer -Rtsne is not optimized for distance calculations and will take a long time to run.

I think the plot would look similar to when perplexity is 1. It might not be as evenly distributed in a spherical shape, but we won't get clean clusters like in part c. 

## f.) Plot iter_cost (KL divergence) against perplexity. What is the optimal perplexity value from the set of perplexities above? Why?

Optimal value of perplexity is at 160 because that had the lowest KL divergence. 
```{r}
plot(perp, temp, 'l')
```

## g.) Plot the embeddings for eta=(10,100,200) while keeping max_iter and your optimal perplexity value selected above constant. What do you notice?

The learning rate affects tSNE is two ways. First, the clusters seem to be better separated and formed. Secondly, it also rotates and shifts the clusters around. 
```{r}
pep <- 160
plot_emb <- function(embedding, i){
  # Wrapper to plot embedding.
  embedding_plot = tibble(x = embedding$Y[,1], y = embedding$Y[,2], 
                        labels = as.character(first_10k_samples_labels))
  ggplot(aes(x = x, y=y,label = labels, color = labels), data = embedding_plot) + 
    geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"') +
    ggtitle(paste0("Learning rate: ",i))
}

i = 10
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                perplexity = pep, 
                theta = 0.5, 
                eta = i, # learning rate. 
                pca = TRUE, verbose = TRUE, 
                max_iter = 500, # number of iterations.
                num_threads=0) 
plot_emb(embedding, i)

i = 100
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                perplexity = pep, 
                theta = 0.5, 
                eta = i, # learning rate. 
                pca = TRUE, verbose = TRUE, 
                max_iter = 500, # number of iterations.
                num_threads=0) 
plot_emb(embedding, i)

i = 200
embedding = Rtsne(X = first_10k_samples, dims = 2, 
                perplexity = pep, 
                theta = 0.5, 
                eta = i, # learning rate. 
                pca = TRUE, verbose = TRUE, 
                max_iter = 500, # number of iterations.
                num_threads=0) 
plot_emb(embedding, i)
```

# Part 2 word-2-vec and Gaussian processes. 
```{r}
library(wordVectors)
library(Rtsne)
library(tidytext)
library(tidyverse)
```

## 2.) Run through the starter code set and try different ingredients, list your ingredients and the top 5 closest to them. They cannot be the same as presented in the starter code. Mark anything that is interesting.

```{r}
if (!file.exists("cookbooks.zip")) {
  download.file("http://archive.lib.msu.edu/dinfo/feedingamerica/cookbook_text.zip","cookbooks.zip")
}
unzip("cookbooks.zip",exdir="cookbooks")
if (!file.exists("cookbooks.txt")) prep_word2vec(origin="cookbooks",destination="cookbooks.txt",lowercase=T,bundle_ngrams=1)

# Training a Word2Vec model
if (!file.exists("cookbook_vectors.bin")) {
  model = train_word2vec("cookbooks.txt","cookbook_vectors.bin",
                         vectors=100,threads=4,window=6,
                         min_count = 10,
                         iter=5,negative_samples=15)
} else{
    model = read.vectors("cookbook_vectors.bin")
}
```

```{r}
ingredient = 'tamarind'
ingredient_2 = 'chives'
ingredient_3 = 'sugar'
ingredient_4 = 'garlic'
list_of_ingredients = c(ingredient, ingredient_2, ingredient_3, ingredient_4)

model %>% closest_to(model[[list_of_ingredients]], 20)

```

## 3.) Use TSNE to find interesting relationships amongst the different set of ingredients that you have selected. 

```{r}
# We have a list of potential herb-related words from old cookbooks. 
n_words = 100
# selecting 100 closest ingredients to sage, thyme, basil.
# model[[list_of_ingredients]] returns a vector that is the average of the ingredient vectors.
closest_ingredients = closest_to(model,model[[list_of_ingredients]], n_words)$word
surrounding_ingredients = model[[closest_ingredients,average=F]] # don't return average

embedding = Rtsne(X = surrounding_ingredients, dims = 2, 
                  perplexity = 4, 
                  theta = 0.5, 
                  eta = 10,
                  pca = TRUE, verbose = TRUE, 
                  max_iter = 2000)
embedding_vals = embedding$Y
rownames(embedding_vals) = rownames(surrounding_ingredients)

# Looking for clusters for embedding
set.seed(10)
n_centers = 10
clustering = kmeans(embedding_vals,centers=n_centers,
                    iter.max = 5)

# Setting up data for plotting
embedding_plot = tibble(x = embedding$Y[,1], 
                        y = embedding$Y[,2],
                        labels = rownames(surrounding_ingredients)) %>% 
  bind_cols(cluster = as.character(clustering$cluster))

# Visualizing TSNE output
ggplot(aes(x = x, y=y,label = labels, color = cluster), data = embedding_plot) + 
  geom_text() +xlab('tSNE dimension 1') +ylab('tSNE dimension 2"')+theme(legend.position = 'none')
```

## 4.) Replace the set of 'tastes' by 3 other orthogonal dimensions. Before selecting your three words, explore the set of words in the data set. Generate the plot for the set of words. Do they make sense?

Looking at the image and the poorly printed output of values, "egg" is roughly orthogonal to "juice" and "cake". Also, "cake" is roughly orthogonal to "juice". 
```{r}
temp <- cosineSimilarity(model[[100:120,]], model[[100:120,]])
image(temp)
temp
```

```{r}
# We can plot along mltiple dimensions:
tastes = c("egg","juice","cake")
common_similarities_tastes = model[1:3000,]%>% cosineSimilarity( model[[tastes,average=F]])
high_similarities_to_tastes = common_similarities_tastes[rank(-apply(common_similarities_tastes,1,max)) < 20,]

# - Plotting
high_similarities_to_tastes %>% 
  as_tibble(rownames='word') %>%
  filter( ! (is.element(word,tastes))) %>%
  #mutate(total = salty+sweet+savory+bitter+sour) %>%
  #mutate( sweet=sweet/total,salty=salty/total,savory=savory/total,bitter=bitter/total, sour = sour/total) %>% 
  #select(-total) %>%
  gather(key = 'key', value = 'value',-word) %>%
  ggplot(aes(x = word,
             y = value, 
             fill = key)) + geom_bar(stat='identity') + 
  coord_flip() + theme_minimal() + scale_fill_brewer(palette='Spectral')
``` 

## 5.) Try different combinations for analogic reasoning. (Given the data is not munged, I expect that some relations will be odd)

```{r}
# We can plot along mltiple dimensions:
tastes = c("cow","sheep")
common_similarities_tastes = model[1:3000,]%>% cosineSimilarity( model[[tastes,average=F]])
high_similarities_to_tastes = common_similarities_tastes[rank(-apply(common_similarities_tastes,1,max)) < 20,]

# - Plotting
high_similarities_to_tastes %>% 
  as_tibble(rownames='word') %>%
  filter( ! (is.element(word,tastes))) %>%
  #mutate(total = salty+sweet+savory+bitter+sour) %>%
  #mutate( sweet=sweet/total,salty=salty/total,savory=savory/total,bitter=bitter/total, sour = sour/total) %>% 
  #select(-total) %>%
  gather(key = 'key', value = 'value',-word) %>%
  ggplot(aes(x = word,
             y = value, 
             fill = key)) + geom_bar(stat='identity') + 
  coord_flip() + theme_minimal() + scale_fill_brewer(palette='Spectral')
```

```{r}
# We can plot along mltiple dimensions:
tastes = c("hand","foot")
common_similarities_tastes = model[1:3000,]%>% cosineSimilarity( model[[tastes,average=F]])
high_similarities_to_tastes = common_similarities_tastes[rank(-apply(common_similarities_tastes,1,max)) < 20,]

# - Plotting
high_similarities_to_tastes %>% 
  as_tibble(rownames='word') %>%
  filter( ! (is.element(word,tastes))) %>%
  #mutate(total = salty+sweet+savory+bitter+sour) %>%
  #mutate( sweet=sweet/total,salty=salty/total,savory=savory/total,bitter=bitter/total, sour = sour/total) %>% 
  #select(-total) %>%
  gather(key = 'key', value = 'value',-word) %>%
  ggplot(aes(x = word,
             y = value, 
             fill = key)) + geom_bar(stat='identity') + 
  coord_flip() + theme_minimal() + scale_fill_brewer(palette='Spectral')
```

```{r}
# We can plot along mltiple dimensions:
tastes = c("apple","orange")
common_similarities_tastes = model[1:3000,]%>% cosineSimilarity( model[[tastes,average=F]])
high_similarities_to_tastes = common_similarities_tastes[rank(-apply(common_similarities_tastes,1,max)) < 20,]

# - Plotting
high_similarities_to_tastes %>% 
  as_tibble(rownames='word') %>%
  filter( ! (is.element(word,tastes))) %>%
  #mutate(total = salty+sweet+savory+bitter+sour) %>%
  #mutate( sweet=sweet/total,salty=salty/total,savory=savory/total,bitter=bitter/total, sour = sour/total) %>% 
  #select(-total) %>%
  gather(key = 'key', value = 'value',-word) %>%
  ggplot(aes(x = word,
             y = value, 
             fill = key)) + geom_bar(stat='identity') + 
  coord_flip() + theme_minimal() + scale_fill_brewer(palette='Spectral')
```

## 6.) Given the set of analogic words and the values that are close to them, what are some interesting relations you find? List 2-3 of them.

Whey is more closely associated with cow than sheep, which makes sense bcause of whey protein and whey milk. Pudding is closer to apple than orange is weird because there is no apple pudding or orange pudding. Lastly, flower is more associated to orange than apple, which makes no sense and should be closely orthogonal. 

## 7.) The starter code sets bundle_ngram=1, which means we are interested in single word embeddings and not common pairs, if we set bundle_ngrams=2 what common pairs of words do you get - list 10. 

```{r}
# -- Check to see  if file exists --
if (!file.exists("cookbooks.zip")) {
  download.file("http://archive.lib.msu.edu/dinfo/feedingamerica/cookbook_text.zip","cookbooks.zip")
}
unzip("cookbooks.zip",exdir="cookbooks")
if (!file.exists("cookbooks2.txt")) prep_word2vec(origin="cookbooks",destination="cookbooks2.txt",lowercase=T,bundle_ngrams=2)
```
```{r}
# Training a Word2Vec model
if (!file.exists("cookbook_vectors2.bin")) {
  model = train_word2vec("cookbooks2.txt","cookbook_vectors2.bin",
                         vectors=100,threads=4,window=6,
                         min_count = 10,
                         iter=5,negative_samples=15)
} else{
    model = read.vectors("cookbook_vectors2.bin")
    }
```


# Question 4 Gaussian Processes
## Part 1 
## 1.) Fit a model using the radial basis function to the data kernel_regression_1.csv
```{r}
library(MASS)
library(ggplot2)
library(dplyr)
library(tidyr)
```

```{r}
data <- read.csv("kernel_regression_1.csv")

```

```{r}
# Fitting a 0 mean gaussian process. 
# Kernel matrix
# Radial basis function. 
K = function(x,x_prime,l){
  d = sapply(x, FUN = function(x_in)(x_in - x_prime)^2)
  return(t(exp(-1/(2*l^2) *d))) # t() is transpose function.
}

sampling_from_a_gp <- function(theta=1){
  # Generating Data
  set.seed(12345)
  n = 500
  x_observed = data$x  # Training data. 
  x_prime = seq(-10, 10,length.out = n)  # vector of new points. 
  f = data$y
  
  # Setting up GP
  mu = mean(f) # note that mean(f) approximately 0.
  mu_star = 0
  
  # Covariance of f
  K_f = K(x_observed,x_observed,theta) + diag(var(f), length(x_observed))
  
  # Marginal and conditional covariance of f_star|f (posterior predictive)
  K_star = K(x_observed,x_prime,theta)
  K_starstar = K(x_prime,x_prime,theta)
  
  # Conditional distribution  f_star|f (Posterior predictive)
  # Page 77 in lecture notes, x = f-mu. It isn't stated, but this is what it represents. 
  # $ E[f_{star}|f] = K_{star}^T * K^{-1} * (f-mu)
  mu_star = mu_star + t(K_star) %*% solve(K_f) %*% (f - mu)
  Sigma_star = K_starstar - t(K_star)%*% t(solve(K_f)) %*% K_star
  
  # Re-arranging values for plotting
  plot_gp = tibble(x = x_prime, 
                   y = mu_star %>% as.vector(),  # This is the mean surface function. 
                   sd_prime = sqrt(diag(Sigma_star)))
  
  # Calculate negative log likelihood
  likelihood = (-1/2) * f %*% solve(K_f) %*% f - (1/2) * log(det(K_f)) - (length(x_observed)/2) * log(2*pi)
  
  # Plotting values
  # The plot in lecture notes does not match this plot. There is a mistake in the lecture notes. 
  # The shaded region is 1 standard deviation of f(x), above and below. 
  # Ploting the mean surface function as a function of x. 
  p=ggplot(aes(x = x, y = y), data = plot_gp) + 
    geom_line()+ 
    geom_ribbon(aes(ymin = y - (2 * sd_prime),ymax = y + (2 * sd_prime)), alpha = 0.2)+
    geom_point(aes(x =x , y= y), data = tibble(x = x_observed, y = f), 
               color = 'red') + # red is the actual data points. 
    xlim(c(-10,10))+ylim(c(-5,5))+
    coord_fixed(ratio = 1) +ylab('f(x)')
  return(list('likelihood' = likelihood, 'plot' = p))
}
```
```{r}
sampling_from_a_gp(theta=2)$p
```

## 2.) Find the optimal value of the hyperparameter $\theta$ by plotting the negative log likelihood and selecting the value that minimizes the negative log likelihood.

Around 15 is a good number because the performance starts to plateau. 
```{r}

t <- seq(1, 30, 2)
out <- c()
for(i in t){
  a <- sampling_from_a_gp(theta=i)
  out[i] <- a$likelihood
  
}

```

```{r}
plot(out)
```

## 3.) Plot the mean function with a 95% confidence interval. 

```{r}
sampling_from_a_gp(theta=15)$p
```

## Part 2
## It is possible to use Gaussian Processes in time series analysis. We do that by combining several kernels that reflect global and local periodicity along iwth macro trends. Reference the kernels in assignment pdf.
## 1.) Construct a GP with $x \in R$ and the covariance below:
## $$ Cov(x,x') = k_{Per} (x,x') + k_{LocalPer}(x,x') + k_{Lin}(x,x') $$

```{r}
k_per = function(x, x_prime){
  p = 2
  l = 2
  var = 4
  return(sapply(x, FUN = function(x_in)(var * exp((-2/(l^2)) * (sin(pi * abs(x_in - x_prime)/p))^2))))
}

k_localper = function(x, x_prime){
  p = 2
  l = 2
  var = 4
  return(sapply(x, FUN = function(x_in)(var * exp((-2/(l^2)) * (sin(pi * abs(x_in - x_prime)/p))^2) * exp(-(1/(2&l^2)) * (x_prime - x_in)^2))))
}

k_lin = function(x, x_prime){
  var_b = 4
  var_c = 3
  c = .5
  return(sapply(x, FUN = function(x_in)(var_b + var_c*(x_in - .5)*(x_prime - .5))))
} 

sampling_from_a_gp = function(x_min = 0, 
                              x_max=1,
                              kernel_in,
                              n = 50, 
                              n_gps = 10){
  # n_gps is the number of gaussian processes to simulate
  # n is the number of observed points in each gaussian process. 
  # Simulation
  x = seq(x_min, x_max,length.out = n)
  K = k_localper(x, x) + k_per(x, x) + k_lin(x, x)
  L = chol(K + 1e-6*diag(n))
  # $ Y ~ \mu + L * N(0,1)
  # matrix(rnorm(n*n_gps), ncol = n_gps) is creating a multivariate N(0,I) with 0 covariance
  f_prior = t(L) %*% matrix(rnorm(n*n_gps), ncol = n_gps)
  
  # Reshaping
  colnames(f_prior) = paste0('Simulation ', seq(1:n_gps))  
  f_prior_long_format = f_prior %>% as_tibble() %>% 
    bind_cols(x = x) %>% 
    pivot_longer(cols = starts_with("sim"))
  
  # Plot
  ggplot(aes(x = x, color = name, y = value),
             data = f_prior_long_format) + 
    geom_line()+theme(legend.position = 'bottom')+
    guides(color=guide_legend(title=""))+
    ylab('f(x)')
}
```

## 2.) Generate and plot 10 simulations over the domain $$x \in (0,10)$$
```{r}
sampling_from_a_gp(x_min=0, x_max=10, n_gps = 10, n = 1000)

```

## 3.) Simulate a set of points with local and global periodicity and fit your model to the data.

Did not optimize the hyperparameters. 
```{r}
x <- seq(0, 30, length.out=500)
# frequency of 1/10 and frequency of 5 
y <- sin(2*pi*x/10) + sin(2*pi*x*5) + rnorm(length(x), 0, 3)
```

```{r}
# Fitting a 0 mean gaussian process. 
sampling_from_a_gp <- function(theta=1){
  # Generating Data
  set.seed(12345)
  n = 1000
  x_observed = x  # Training data. 
  x_prime = seq(0, 50,length.out = n)  # vector of new points. 
  f = y
  
  # Setting up GP
  mu = mean(f) # note that mean(f) approximately 0.
  mu_star = 0
  
  # Covariance of f
  K_f = k_localper(x_observed, x_observed) + k_per(x_observed, x_observed) + k_lin(x_observed, x_observed) + diag(var(f), length(x_observed))
  
  # Marginal and conditional covariance of f_star|f (posterior predictive)
  K_star = t(k_localper(x_observed, x_prime) + k_per(x_observed, x_prime) + k_lin(x_observed, x_prime))
  K_starstar = k_localper(x_prime, x_prime) + k_per(x_prime, x_prime) + k_lin(x_prime, x_prime)
  
  # Conditional distribution  f_star|f (Posterior predictive)
  # Page 77 in lecture notes, x = f-mu. It isn't stated, but this is what it represents. 
  # $ E[f_{star}|f] = K_{star}^T * K^{-1} * (f-mu)
  mu_star = mu_star + t(K_star) %*% solve(K_f) %*% (f - mu)
  Sigma_star = K_starstar - t(K_star)%*% t(solve(K_f)) %*% K_star
  
  # Re-arranging values for plotting
  plot_gp = tibble(x = x_prime, 
                   y = mu_star %>% as.vector(),  # This is the mean surface function. 
                   sd_prime = sqrt(diag(Sigma_star)))
  
  # Calculate negative log likelihood
  likelihood = (-1/2) * f %*% solve(K_f) %*% f - (1/2) * log(det(K_f)) - (length(x_observed)/2) * log(2*pi)
  
  # Plotting values
  # The plot in lecture notes does not match this plot. There is a mistake in the lecture notes. 
  # The shaded region is 1 standard deviation of f(x), above and below. 
  # Ploting the mean surface function as a function of x. 
  ggplot(aes(x = x, y = y), data = plot_gp) + 
    geom_line()+ 
    geom_ribbon(aes(ymin = y - (2 * sd_prime),ymax = y + (2 * sd_prime)), alpha = 0.2)+
    geom_point(aes(x =x , y= y), data = tibble(x = x_observed, y = f), 
               color = 'red') + # red is the actual data points. 
    xlim(c(0,50))+ylim(c(-10,10))+
    coord_fixed(ratio = 1) +ylab('f(x)')
}

```

```{r}
sampling_from_a_gp()
```


























